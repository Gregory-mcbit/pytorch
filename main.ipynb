{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "PyTorch is a deep learning framework used for research and development in machine learning and artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor is a fundamental data structure that is similar to arrays or matrices. Tensors are the building blocks of neural networks and are used to represent data in the form of multi-dimensional arrays\n",
    "\n",
    "### Types of Tensors\n",
    "![](./Tensor.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.6731e-01,  4.1031e-01, -1.5540e-01,  ...,  2.0594e-01,\n",
       "          -7.7255e-01, -1.1984e-01],\n",
       "         [-5.6662e-01,  7.0067e-01, -3.4180e-01,  ..., -1.1127e+00,\n",
       "           1.8683e+00, -1.1175e+00],\n",
       "         [ 8.8551e-01, -6.9390e-01,  6.1661e-01,  ...,  9.9827e-01,\n",
       "           5.2346e-01,  1.1848e-01],\n",
       "         ...,\n",
       "         [-1.3863e+00, -1.5750e+00, -9.7003e-01,  ..., -2.8985e-01,\n",
       "           1.1005e+00, -3.1892e-01],\n",
       "         [-4.7203e-01, -2.4616e-01, -1.9265e+00,  ..., -7.0816e-01,\n",
       "          -5.6475e-01, -8.3122e-01],\n",
       "         [-1.1669e+00, -1.0620e-03, -1.9500e-02,  ...,  7.5206e-01,\n",
       "           1.8913e+00,  6.1879e-01]],\n",
       "\n",
       "        [[ 4.6298e-01,  5.4132e-01, -6.9952e-01,  ..., -4.9100e-01,\n",
       "          -7.0300e-01, -6.1826e-01],\n",
       "         [-8.0461e-01, -6.8585e-01, -5.3335e-01,  ..., -5.4940e-02,\n",
       "          -2.2417e-01, -9.4653e-01],\n",
       "         [ 1.5007e+00, -9.9051e-01,  1.6570e-02,  ..., -3.8530e-01,\n",
       "          -2.6853e-01, -4.9141e-01],\n",
       "         ...,\n",
       "         [ 8.4170e-01,  5.9050e-01,  1.0565e+00,  ...,  1.1243e+00,\n",
       "           2.5168e-01, -1.5642e+00],\n",
       "         [ 8.0263e-01, -1.9716e+00, -1.3334e+00,  ...,  2.2510e+00,\n",
       "           2.0635e-01, -2.1207e-01],\n",
       "         [ 6.3797e-01, -7.9496e-01, -2.5321e-01,  ..., -3.2946e-01,\n",
       "          -4.7364e-01, -2.7030e-01]],\n",
       "\n",
       "        [[-3.0399e-01,  1.5003e+00, -2.1852e-01,  ...,  5.6885e-02,\n",
       "           1.5649e+00, -5.1509e-01],\n",
       "         [ 4.6452e-01,  3.7256e-01,  3.8977e-01,  ..., -1.2080e+00,\n",
       "           1.0121e+00, -1.5772e+00],\n",
       "         [ 1.3017e-01, -9.8137e-01, -3.1028e-01,  ...,  7.8474e-01,\n",
       "          -3.5978e-02,  2.8495e-01],\n",
       "         ...,\n",
       "         [ 7.2386e-01,  2.1574e-02,  8.7633e-01,  ..., -5.2339e-01,\n",
       "          -1.3498e+00,  6.1217e-01],\n",
       "         [ 6.8984e-01,  3.0858e-01,  1.1412e+00,  ...,  1.8345e+00,\n",
       "          -9.4012e-01,  2.7153e+00],\n",
       "         [ 7.8942e-01, -3.3977e-01,  9.7477e-01,  ...,  1.6442e-01,\n",
       "          -3.7046e-02, -1.5791e+00]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar = torch.tensor(42.0)  # Creates a scalar tensor with the value 42.0. has 0 dimensions\n",
    "vector = torch.tensor([1, 2, 3, 4, 5])  # Creates a 1-D tensor with 5 elements\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Creates a 2-D tensor with 2 rows and 3 columns\n",
    "four_dim_tensor = torch.randn(32, 3, 64, 64)  # Create a 4-D tensor with shape (batch_size, channels, height, width)\n",
    "four_dim_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different arguments can be provided for tensor creation:\n",
    "* Data\n",
    "* Dtype\n",
    "* Device (specify the device (CPU or GPU) on which the tensor should be located using this argument. If not provided, the tensor will be created on the CPU by default.)\n",
    "* Requires_grad (If set to True, the tensor will be set up to track operations on it for automatic differentiation (autograd) during backpropagation. This is useful for gradient-based optimization and training deep learning models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor(data=[[1, 2, 3], [4, 5, 6]], \n",
    "dtype=torch.float32, \n",
    "device='cpu', \n",
    "requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor(data=[[1, 2, 3], [3, 2, 3]])\n",
    "tensor.type(torch.float32)\n",
    "tensor.numel()  # total elements in tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 3],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_tensor = torch.reshape(tensor, (3, 2))\n",
    "reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 3],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_tensor = torch.reshape(tensor, (-1, 2))  # -1 is used to infer one of dimensions\n",
    "reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1679, -0.4802,  0.6324,  ...,  0.7223, -0.5063, -0.7060],\n",
       "        [ 1.2336,  0.3826,  0.2796,  ..., -0.2616,  0.6765, -0.0804],\n",
       "        [-1.3299,  0.9156, -0.3471,  ...,  0.0298, -0.8025,  0.7965],\n",
       "        ...,\n",
       "        [ 0.9928, -0.0565, -0.2597,  ..., -0.2338,  0.4734,  0.0774],\n",
       "        [-0.2038, -0.8475, -1.2998,  ...,  2.0775,  0.3445,  0.5881],\n",
       "        [-0.6292, -2.4918,  1.1873,  ...,  0.4216,  2.0810,  1.0084]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(32, 3, 64, 64)\n",
    "x_flattened = x.view(x.size(0), -1)\n",
    "x_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_tensor = torch.unsqueeze(tensor, dim=0)  #  Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "expanded_tensor.size(), tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute function\n",
    "The permute() function allows to rearrange dimensions in a tensor, providing with the flexibility to change the shape and orientation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permuted_tensor = tensor.permute(1, 0)  # Swap dimensions 0 and 1\n",
    "permuted_tensor.shape, tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3],\n",
       "         [3, 2, 3]]),\n",
       " tensor([[1, 3],\n",
       "         [2, 2],\n",
       "         [3, 3]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor, permuted_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original tensor shape is: torch.Size([2, 3]),\n",
      "The transposed tensor using .t shape is: torch.Size([3, 2]),\n",
      "The transposed tensor using .tranpose shape is: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Transposing a Tensor (Swapping Rows and Columns)\n",
    "transposed_tensor_1 = tensor.t()\n",
    "transposed_tensor_2 = torch.transpose(tensor, 0, 1)  # Swap axes 0 and 1\n",
    "\n",
    "print(f'The original tensor shape is: {tensor.shape},\\n' \n",
    "      f'The transposed tensor using .t shape is: {transposed_tensor_1.shape},\\n' \n",
    "      f'The transposed tensor using .tranpose shape is: {transposed_tensor_2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition and subtraction between tensors same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9,  9, 10],\n",
       "        [17, 17,  7]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[4, 5, 7], [8, 9, 0]])\n",
    "tensor_b = torch.tensor([[5, 4, 3], [9, 8, 7]])\n",
    "\n",
    "tensor_a + tensor_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1,  1,  4],\n",
       "        [-1,  1, -7]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a - tensor_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise multiplication between 2 tensors of same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 20, 21],\n",
       "        [72, 72,  0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a * tensor_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-wise multiplication (dot product) between 2 tensors where the inner dimensions match (the number of columns in the first tensor equals the number of rows in the second tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 72,  63,  54],\n",
       "        [121, 104,  87]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_c = torch.tensor([[5, 4, 3], [9, 8, 7], [1, 1, 1]])\n",
    "matmu = torch.matmul(tensor_a, tensor_c)\n",
    "matmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8000, 1.2500, 2.3333],\n",
       "        [0.8889, 1.1250, 0.0000]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div = tensor_a / tensor_b\n",
    "div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     1024,       625,       343],\n",
       "        [134217728,  43046721,         0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_exp = tensor_a ** tensor_b\n",
    "result_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 2.2361, 2.6458],\n",
       "        [2.8284, 3.0000, 0.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_sqrt = torch.sqrt(tensor_a)\n",
    "result_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3863, 1.6094, 1.9459],\n",
       "        [2.0794, 2.1972,   -inf]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_log = torch.log(tensor_a)  # natural logarithm (base e)\n",
    "result_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(33.),\n",
       " tensor([5.3333, 5.6667]),\n",
       " torch.return_types.max(\n",
       " values=tensor([8., 9., 7.]),\n",
       " indices=tensor([1, 1, 0])),\n",
       " torch.return_types.min(\n",
       " values=tensor([4., 0.]),\n",
       " indices=tensor([0, 2])))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a = tensor_a.type(torch.float32)\n",
    "total_sum = torch.sum(tensor_a)\n",
    "\n",
    "# Compute the mean along axis 1 (rows)\n",
    "mean_along_rows = torch.mean(tensor_a, dim=1)\n",
    "\n",
    "# Compute the maximum value along axis 0 (columns)\n",
    "max_along_columns = torch.max(tensor_a, dim=0)\n",
    "\n",
    "# Compute the minimum value along axis 1 (rows)\n",
    "min_along_rows = torch.min(tensor_a, dim=1)\n",
    "\n",
    "total_sum, mean_along_rows, max_along_columns, min_along_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting in PyTorch\n",
    "\n",
    "The key idea behind broadcasting is that the smaller tensor is \"broadcasted\" or expanded to match the shape of the larger one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcast results is: tensor([[ 6.,  7.,  9.],\n",
      "        [10., 11.,  2.]]) and of shape torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "scalar = 2\n",
    "result_broadcast = tensor_a + scalar\n",
    "print(f'broadcast results is: {result_broadcast} and of shape {result_broadcast.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two 2x2 tensors, tensor_a and tensor_b, and we want to concatenate them along dimension 0 to create a new tensor with a shape of 4x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated tensor is: tensor([[2, 2],\n",
      "        [2, 2],\n",
      "        [4, 4],\n",
      "        [4, 4]]) and of shape torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "tensor_a, tensor_b = torch.tensor([[2, 2], [2, 2]]), torch.tensor([[4, 4], [4, 4]])\n",
    "concatenated_tensor = torch.cat((tensor_a, tensor_b), dim=0)\n",
    "print(f'concatenated tensor is: {concatenated_tensor} and of shape {concatenated_tensor.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGrad and Gradients\n",
    "\n",
    "Autograd, short for Automatic Differentiation, is a key feature of PyTorch that allows for automatic computation of gradients (derivatives) of tensors. It is an essential component for training deep learning models through backpropagation.\n",
    "1. **Gradient Calculation** - In deep learning, we often need to compute gradients of a loss function with respect to model parameters. Autograd simplifies this process. When you perform operations on tensors that require gradients, PyTorch automatically tracks these operations and constructs a computation graph.\n",
    "\n",
    "2. **Computation Graph** - A computation graph is a directed acyclic graph (DAG) that represents the sequence of operations applied to tensors. Each operation in the graph is a node, and tensors flowing through these nodes are edges. The graph allows PyTorch to trace how input tensors influence the output tensors, which is crucial for gradient calculation.\n",
    "\n",
    "3. **Dynamic Computational Graph** - PyTorch uses a dynamic computation graph, which means the graph is built on-the-fly as operations are executed. This dynamic nature allows flexibility and is well-suited for models with varying architectures or inputs of different shapes.\n",
    "\n",
    "4. **Gradients** - Once you have a computation graph, you can compute gradients by backpropagating through the graph. Gradients represent how a small change in each input tensor would affect the final output. The gradients are computed using the chain rule of calculus, and they indicate the direction and magnitude of parameter updates during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([3.0, 2.0, 3.0], requires_grad=True)  # start tracking gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass. PyTorch records these operations in the computation graph\n",
    "y = x * 2\n",
    "z = y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "To compute gradients, initiate the backward pass using the backward() method on a scalar tensor (usually a loss)\n",
    "\n",
    "Chain Rule: backThe ward pass uses the chain rule of calculus to calculate the gradients. It starts from the final scalar value z and works backward through the computation graph to compute the gradients of intermediate tensors with respect to the target tensor (x in this case).\n",
    "\n",
    "It computes ∂z/∂y, which is the gradient of z with respect to y. Then, it computes ∂y/∂x, which is the gradient of y with respect to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the backward pass is stored in the .grad attribute of the tensors with requires_grad=True. In this case, x.grad will contain the gradient of z with respect to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with Autograd: 36.0\n",
      "This tensor does't have require gradients set to True\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with Autograd enabled (requires_grad=True)\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Perform some operations with Autograd enabled\n",
    "y = x * 3\n",
    "z = y ** 2\n",
    "w = z.mean()\n",
    "\n",
    "# Compute gradients while Autograd is enabled\n",
    "w.backward()\n",
    "\n",
    "# Access the gradient of x\n",
    "gradient_with_autograd = x.grad\n",
    "\n",
    "# Print the gradient\n",
    "print(\"Gradient with Autograd:\", gradient_with_autograd.item())\n",
    "\n",
    "\n",
    "# Now, let's turn Autograd off for a specific tensor\n",
    "x.requires_grad_(False)\n",
    "\n",
    "# Perform operations without Autograd (Autograd is off for x)\n",
    "y = x * 3\n",
    "z = y ** 2\n",
    "w = z.mean()\n",
    "try:\n",
    "    # Attempt to compute gradients \n",
    "    w.backward()\n",
    "except:\n",
    "    print(\"This tensor does't have require gradients set to True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Parameter\n",
    "\n",
    "In PyTorch, nn.Parameter is a class that is a subclass of the torch.Tensor class. It is specifically designed to be used as a container for tensors that should be considered parameters of a PyTorch nn.Module. Parameters are tensors that are meant to be learned during the training process, such as weights and biases in a neural network.\n",
    "\n",
    "Why nn.Parameter is useful?\n",
    "\n",
    "* Requires Grad Calculation: When you create a tensor using nn.Parameter, it is automatically registered as a parameter of the parent module, and PyTorch keeps track of it for gradient computation during backpropagation. This means that any operations involving these tensors will have gradients computed, allowing them to be updated during training using optimization techniques like stochastic gradient descent (SGD).\n",
    "* Initialization: Parameters created using nn.Parameter are typically initialized with random values (e.g., Gaussian or uniform distribution) by default. However, you can customize the initialization method if needed.\n",
    "* Access: You can easily access the parameters of a PyTorch module using the parameters() method, which returns an iterable containing all the nn.Parameter objects within the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3426,  0.2997,  0.0477,  0.9563, -0.8480],\n",
      "        [-2.2974, -1.8996,  0.8777,  0.4756, -1.2931],\n",
      "        [-0.3829,  0.3655, -0.4606, -2.3426,  0.7322],\n",
      "        [-0.0354,  0.4037, -0.9113,  1.4786,  0.5362],\n",
      "        [-0.0795,  0.3935,  1.7341, -0.0545,  0.2704],\n",
      "        [-1.1718,  0.6017, -0.2846, -0.2442,  1.0230],\n",
      "        [-0.8148,  0.9376,  0.7216, -1.5338,  0.6573],\n",
      "        [ 0.3531, -0.1765,  0.6951,  2.3895,  0.3509],\n",
      "        [-0.1698, -0.3017, -0.5952,  0.6879,  0.5409],\n",
      "        [-0.1112, -1.4798, -1.2071, -0.5397,  1.2772]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Create an nn.Parameter for weight and bias\n",
    "        self.weight = nn.Parameter(torch.randn(10, 5))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the parameters in the forward pass\n",
    "        z = torch.matmul(x, self.weight.t()) + self.bias\n",
    "        return z\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel()\n",
    "\n",
    "# Access and print the parameters\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
